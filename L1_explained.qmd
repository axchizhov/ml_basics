---
title: "Под капотом L1 регуляризации"
subtitle: "Почему она отбирает признаки? От теории к практике"
date: "now"
lang: ru
toc: true
format:
  # ipynb: default
  html:
    code-fold: true
jupyter: python3
execute:
  eval: true
---

# Введение

L1 регуляризация умеет отбирать признаки. Это ее свойство почти повсеместно объясняют с помощью статистических рассуждений и варианта этой картинки:

![Image from Elements of Statistical Learning by Hastie, Tibshirani, and Friedman](l1_fig.png)

У меня и правда возникло смутное чувство интуитивного понимания, когда я эти рассуждения читал и смотрел на картинку выше. Но когда я начал разбираться конкретно, это чувство довольно быстро улетучилось.

Ниже я опишу свою попытку разобраться как L1 действительно работает на практике. Для простоты будет разбирать все на основе линейной регрессии со среднеквадратичной ошибкой (lasso regression). Для более сложных моделей рассуждения, в принципе, будут такими же.

Предлагаю вам проследовать за моими рассуждениями. Ну или сразу [прыгнуть до ключевой идеи](#sec-bottom-line).


# Определяем L1

Итак, чтобы регуляризовать линейную регрессию, мы просто добавляем к нашей ошибке штраф, который зависит от абсолютного размера весов:

$$ \text{MSE Loss}+ L_1 = \frac{1}{2n} ||Xw - y||_{2}^{2} + \alpha ||w||_1 $$

$$ ||w||_1 = |w_1| + |w_2| + \ldots $$

Для нахождения оптимальных весов нам, как обычно, потребуются градиенты:

$$ \nabla_w \text{MSE Loss} = \frac{1}{n} X^T (Xw - y) $$

$$ \nabla_w L_1 = \alpha \cdot \text{sign}(w) $$

\begin{equation}
\text{sign}(w_i) = \begin{cases} 
-1 & \text{if } w_i < 0 \\
0 & \text{if } w_i = 0 \\
1 & \text{if } w_i > 0 
\end{cases}
\end{equation}

Тут может возникнуть вопрос: а как это оптимизировать? У L1 разрыв производной в нуле и, формально, градиентный спуск здесь применять нельзя. Ну а мы все равно попробуем.


# Спускаемся по градиенту

Сначала сгенерируем простенький датасет:

* 50 объектов
* 1 релевантный признак
* 4 признака со случайным шумом

```{python}
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression

sns.set()
np.set_printoptions(suppress=True)

np.random.seed(42)

# Generate the data
X, y = make_regression(n_samples=50, n_features=5, n_informative=1, noise=0.01, bias=1)
y = y.reshape(-1, 1)

# Convert to DataFrame
data = pd.DataFrame(X, columns=[f'Feature {i}' for i in range(X.shape[1])])
data['Target'] = y

# Plot correlation plots
sns.pairplot(data, y_vars='Target', x_vars=data.columns[:-1], kind='reg')
plt.show()
```


Далее набросаем саму модель. Ее веса будем обновлять следующим образом:

$$w_i := w_i - \lambda (\frac{\partial \text{MSE Loss}}{\partial w_i} +  \alpha \cdot \text{sign}(w_i) ) $$


```{python}
#| code-fold: false

from sklearn.metrics import mean_squared_error

class LinearRegressionNaiveL1:
    def __init__(self, learning_rate, epochs, alpha, n_features):
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.alpha = alpha  # regularization parameter
        # We can init weights with zeros, because we calculated the gradient analytically
        self.weights = np.zeros((n_features,1))
        self.bias = 0
        self.loss_history = []

    def fit(self, X, y):
        n_samples, n_features = X.shape

        for _ in range(self.epochs):
            y_pred = self.predict(X)

            # Compute gradients
            d_l1 = self.alpha * np.sign(self.weights)
            d_weights = (1/(2*n_samples)) * (X.T @ (y_pred - y))
            d_bias = (1/(2*n_samples)) * np.sum(y_pred - y)

            # Update weights and bias
            self.weights -= self.learning_rate * (d_weights + d_l1)
            self.bias -= self.learning_rate * d_bias
            
            mse = mean_squared_error(y, y_pred)            
            self.loss_history.append(mse)

    def predict(self, X):
        return X @ self.weights + self.bias


n_samples, n_features = X.shape
learning_rate = 0.1
epochs = 200
alpha = 5

model = LinearRegressionNaiveL1(learning_rate=learning_rate, epochs=epochs, alpha=alpha, n_features=n_features)
model.fit(X, y)
```

## Результаты обучения

Обучение прошло без проблем, модель даже показывает приличные прогнозы.

::: {layout-ncol=2 .column-page}

```{python}
#| label: fig-naive-loss

plt.plot(model.loss_history)
plt.xlabel('Iteration')
plt.ylabel('Mean Squared Error')
plt.title('Loss Change Over Iterations')
plt.show()

```


```{python}

y_pred = model.predict(X)

plt.scatter(y, y_pred, color='blue', label='Predicted vs True')
plt.plot([min(y), max(y)], [min(y), max(y)], color='red', linestyle='--', linewidth=2, label='Perfect Fit')
plt.xlabel('True Values')
plt.ylabel('Predicted Values')
plt.title('Prediction Error vs True Values')
plt.legend()
plt.show()

```

:::

Модель правильно выцепила релевантный признак и дала ему большой вес. Однако, ни один вес в ноль не обратился:

```{python}
print(model.weights.flatten())
```

Окей, обычный градиентный спуск веса не обнуляет. Как быть?

# Проверим, что общеупотребительное решение работает

Возьмем Л1 из коробки (sklearn) и проверим, какой результат оно нам выдаст.

И тут все работает. Наш $\alpha = 10$ достаточно велик, чтобы обнулить все веса.

```{python}
#| label: fig-sklearn-models
#| fig-cap: "Обучили модели на случайных данных"

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression, Lasso, Ridge

# Fit a linear regression model with L2 regularization
lr = Ridge(max_iter=epochs)
lr.fit(X, y)
weights_lr = lr.coef_.flatten()

# Fit a linear regression model with L1 regularization
lasso = Lasso(max_iter=epochs, alpha=alpha)
lasso.fit(X, y)
weights_lasso = lasso.coef_.flatten()

# Plot the weights for comparison
# plt.figure(figsize=(10, 5))
plt.plot(weights_lr, 'o-', label='Ridge Regression')
plt.plot(weights_lasso, 's-', label='Lasso Regression')
plt.xlabel('Feature index')
plt.ylabel('Weight value')
plt.title('Comparison of Weights (sklearn lib): Ridge Regression vs Lasso Regression')
plt.legend()
plt.grid(True)
plt.show()

# Display the weights
print("Ridge Regression Weights:\n", weights_lr)
print("Lasso Regression Weights:\n", weights_lasso)
```


# Правильная реализация Л1 {#sec-bottom-line}

Почему градиентный спуск не сработал?

Первая и правильная мысль — у Л1 есть разрыв производной в нуле. Формально, градиентный спуск здесь применять нельзя. Но на практике, нет ничего страшного в одном таком разрыве — даже если мы и не попадем в оптимум, мы все равно сможем очень близко подойти к нему. Однако, как раз это и мешает нам получить нулевые веса у фичей.

Окей, что делать? Можно углубиться в теорию оптимизаций выпуклых функций. Как вариант, метод проксимального градиента даст решение этой задаче. Но небанальные математические выкладки на самом деле приведут к весьма простому результату, который можно назвать банальной эвристикой.

Итак, готовы? Ключевая идея Л1 регуляризации:

:::{.callout-note}
## Ключевая идея Л1

Чтобы отбирать признаки при регуляризации, надо обнулять веса у признаков, когда они становятся достаточно маленькими (меньше $\alpha$)
:::

Да-да, 

Л2


нужно обеспечит постепенное уменьшение весов (отнимаем альфу)
л2 тоже это может, для л1 это просто легче делается, потому что мы линейно добавку вычитаем 
любой механизм распада весов позволяет отбирать признаки при модификации спуска

Попробуем?

```{python}
#| code-fold: false
#| 
class LinearRegressionTrueL1:
    def __init__(self, learning_rate, epochs, alpha, n_features):
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.alpha = alpha  # regularization parameter
        self.weights = np.random.randn(n_features, 1) * 0.001
        self.bias = 0
        self.loss_history = []
    
    def make_small_weights_zero(self, weights):
        alpha_scaled = self.learning_rate * self.alpha
        
        clipped_step = np.sign(weights) * np.minimum(np.abs(weights), alpha_scaled)
        
        weights -= clipped_step

        return weights

    def fit(self, X, y):
        n_samples, n_features = X.shape

        for _ in range(self.epochs):
            y_pred = self.predict(X)

            # Compute gradients
            d_l1 = self.alpha * np.sign(self.weights)
            d_weights = (1/(2*n_samples)) * (X.T @ (y_pred - y))
            d_bias = (1/(2*n_samples)) * np.sum(y_pred - y)

            # Update weights and bias
            self.weights -= self.learning_rate * (d_weights + d_l1)
            self.weights = self.make_small_weights_zero(self.weights)
            self.bias -= self.learning_rate * d_bias
            
            mse = mean_squared_error(y, y_pred)            
            self.loss_history.append(mse)

    def predict(self, X):
        return X @ self.weights + self.bias
```

```{python}

model = LinearRegressionTrueL1(learning_rate=learning_rate, epochs=epochs, alpha=0.5, n_features=n_features)

model.fit(X, y)

weights_my = model.weights.reshape(-1)
print(f"My {model} Weights:\n{weights_my}")
print(f"len weights:{len(weights_my)}")
print(f"number of non-zero weights:{sum(weights_my != 0.)}")


plt.figure(figsize=(10, 5))
# plt.stem(weights_lr, linefmt="b-", markerfmt="bo", basefmt=" ", label="Linear Regression")
# plt.stem(weights_lasso, linefmt="r-", markerfmt="rs", basefmt=" ", label="Lasso Regression")
plt.stem(weights_my, linefmt="g-", markerfmt="x", basefmt=" ", label="My Lasso")
plt.xlabel("Feature index")
plt.ylabel("Weight value")
plt.title("Comparison of Weights: Linear Regression vs Lasso Regression")
plt.legend()
plt.grid(True)
plt.show()

```

Ура, получилось.


есть три нюанса:
1. коэффициэнты стратуеют с нуля (для линейной модели можно)
2. используется модификация градиентного спуска
3. ?

## заметки

нулевые признаки будут оставаться в нуле
больше шансов на нахождение локальных оптимумов?

# Выводы


Мы разобрались, что под капотом у Л1 регуляризации довольно простая эвристика. Она работает на практике и действительно позволяет отбирать признаки.

Очевидный нюанс: так как Л1 обнуляет любые веса меньше $\alpha$, то она может отбросить значимые признаки, если они сильно отличаются по масштабу. При использовании этой техники все признаки стоит нормализовывать.



в принципе, спарсити можно обеспечить и другим пенальти, можно даже без учета значений весов
(как только посчитать функцию потерь?)

# Прочие объяснения

Л1 можно объяснять через вероятностный механизм

Обычно Л1 объясняют через вот эти графики, но мне оно не очевидно
<!-- 
```{python}
import numpy as np
import matplotlib.pyplot as plt

# Generate the original meshgrid
x = np.linspace(-2, 2, 800)
y = np.linspace(-2, 2, 800)
X, Y = np.meshgrid(x, y)

# L1 norm
Z1 = np.abs(X) + np.abs(Y)

# L2 norm
Z2 = np.sqrt(X**2 + Y**2)

# Rotation angle in radians
theta = np.pi * 0.25 

# Rotate the coordinates
X_rot = X * np.cos(theta) - Y * np.sin(theta)
Y_rot = X * np.sin(theta) + Y * np.cos(theta)

# MSE with target (a, b)
a, b = 1, 0
Z_mse_rot = (X_rot - a)**2 + (2*Y_rot - b)**2

fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))

# L1 contour plot
ax1.contour(X, Y, Z1, levels=10)
ax1.set_title('L1 Norm Contour Plot')
ax1.set_xlabel('x')
ax1.set_ylabel('y')
ax1.axhline(0, color='black', linewidth=0.5)
ax1.axvline(0, color='black', linewidth=0.5)
ax1.grid(color='gray', linestyle='--', linewidth=0.5)

# L2 contour plot
ax2.contour(X, Y, Z2, levels=10)
ax2.set_title('L2 Norm Contour Plot')
ax2.set_xlabel('x')
ax2.set_ylabel('y')
ax2.axhline(0, color='black', linewidth=0.5)
ax2.axvline(0, color='black', linewidth=0.5)
ax2.grid(color='gray', linestyle='--', linewidth=0.5)

# MSE contour plot
ax3.contour(X, Y, Z_mse_rot, levels=10)
ax3.set_title('MSE Contour Plot')
ax3.set_xlabel('x')
ax3.set_ylabel('y')
ax3.axhline(0, color='black', linewidth=0.5)
ax3.axvline(0, color='black', linewidth=0.5)
ax3.grid(color='gray', linestyle='--', linewidth=0.5)

plt.tight_layout()
plt.show()

plt.show()
```

```{python}
fig, ax = plt.subplots(figsize=(8, 8))

# L1 norm contour plot
ax.contour(X, Y, Z1, levels=[1], colors='b', label='L1 Norm')

# L2 norm contour plot
ax.contour(X, Y, Z2, levels=[1], colors='g', label='L2 Norm')

# MSE contour plot
ax.contour(X, Y, Z_mse_rot, levels=[1], colors='r', label='MSE')

ax.set_title('Contour Plots of L1, L2 Norms, and MSE')
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.axhline(0, color='black', linewidth=0.5)
ax.axvline(0, color='black', linewidth=0.5)
ax.grid(color='gray', linestyle='--', linewidth=0.5)
ax.legend()

plt.tight_layout()
plt.show()

```


```{python}
import numpy as np
import matplotlib.pyplot as plt

# Generate the original meshgrid
x = np.linspace(-2, 2, 800)
y = np.linspace(-2, 2, 800)
X, Y = np.meshgrid(x, y)

# L1 norm
Z1 = np.abs(X) + np.abs(Y)

# L2 norm
Z2 = np.sqrt(X**2 + Y**2)

# Rotation angle in radians
theta = np.pi * 0.25 

# Rotate the coordinates
X_rot = X * np.cos(theta) - Y * np.sin(theta)
Y_rot = X * np.sin(theta) + Y * np.cos(theta)

# MSE with target (a, b)
a, b = 1, 0
Z_mse_rot = (X_rot - a)**2 + (4*Y_rot - b)**2

fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))

# L1 contour plot
ax1.contour(X, Y, Z1, levels=10)
ax1.set_title('L1 Norm Contour Plot')
ax1.set_xlabel('x')
ax1.set_ylabel('y')
ax1.axhline(0, color='black', linewidth=0.5)
ax1.axvline(0, color='black', linewidth=0.5)
ax1.grid(color='gray', linestyle='--', linewidth=0.5)

# L2 contour plot
ax2.contour(X, Y, Z2, levels=10)
ax2.set_title('L2 Norm Contour Plot')
ax2.set_xlabel('x')
ax2.set_ylabel('y')
ax2.axhline(0, color='black', linewidth=0.5)
ax2.axvline(0, color='black', linewidth=0.5)
ax2.grid(color='gray', linestyle='--', linewidth=0.5)

# MSE contour plot
ax3.contour(X, Y, Z_mse_rot, levels=10)
ax3.set_title('MSE Contour Plot')
ax3.set_xlabel('x')
ax3.set_ylabel('y')
ax3.axhline(0, color='black', linewidth=0.5)
ax3.axvline(0, color='black', linewidth=0.5)
ax3.grid(color='gray', linestyle='--', linewidth=0.5)

plt.tight_layout()
plt.show()

plt.show()
```

```{python}
fig, ax = plt.subplots(figsize=(8, 8))

# L1 norm contour plot
ax.contour(X, Y, Z1, levels=[1], colors='b', label='L1 Norm')

# L2 norm contour plot
ax.contour(X, Y, Z2, levels=[1], colors='g', label='L2 Norm')

# MSE contour plot
ax.contour(X, Y, Z_mse_rot, levels=[1], colors='r', label='MSE')

ax.set_title('Contour Plots of L1, L2 Norms, and MSE')
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.axhline(0, color='black', linewidth=0.5)
ax.axvline(0, color='black', linewidth=0.5)
ax.grid(color='gray', linestyle='--', linewidth=0.5)
ax.legend()

plt.tight_layout()
plt.show()

``` -->
