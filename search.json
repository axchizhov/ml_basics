[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Заметки",
    "section": "",
    "text": "Под капотом L1 регуляризации\n\n\nПочему она отбирает признаки?\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "random_generators.html",
    "href": "random_generators.html",
    "title": "Заметки",
    "section": "",
    "text": "опробовать материалы от кнута"
  },
  {
    "objectID": "posts/l1_explained/l1_explained.html",
    "href": "posts/l1_explained/l1_explained.html",
    "title": "Под капотом L1 регуляризации",
    "section": "",
    "text": "L1 регуляризация умеет отбирать признаки. Это ее свойство почти повсеместно объясняют с помощью статистических рассуждений и варианта вот этой иллюстрации от создателей алгоритма:\n\n\n\nImage from Elements of Statistical Learning by Hastie, Tibshirani, and Friedman\n\n\nРазглядывая эту картинку, у меня и правда возникло смутное чувство интуитивного понимания. Но вот когда я попытался объяснить суть алголритма знакомому, эта интуиция куда-то улетучилась.\nПредлагаю вместе со мной разобраться, как L1 действительно отбирает признаки.\nДля наглядности мы будем регуляризовывать линейную регрессию со среднеквадратичной ошибкой (lasso regression).\nМои рассуждения можно пропустить и сразу прыгнуть до ключевой идеи."
  },
  {
    "objectID": "posts/l1_explained/l1_explained.html#результаты-обучения",
    "href": "posts/l1_explained/l1_explained.html#результаты-обучения",
    "title": "Под капотом L1 регуляризации",
    "section": "Результаты обучения",
    "text": "Результаты обучения\nОбучение прошло без проблем, модель даже показывает приличные прогнозы.\n\nКод\nplt.plot(model.loss_history)\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Loss Change Over Iterations')\nplt.show()\ny_pred = model.predict(X)\n\nplt.scatter(y, y_pred, color='blue', label='Predicted vs True')\nplt.plot([min(y), max(y)], [min(y), max(y)], color='red', linestyle='--', linewidth=2, label='Perfect Fit')\nplt.xlabel('True Values')\nplt.ylabel('Predicted Values')\nplt.title('Predicted vs True Values')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nАлгоритм правильно выцепил релевантный признак и дал ему большой вес. Однако, ни один вес в ноль не обратился:\n\n\nКод\nprint(model.weights.flatten())\n\n\n[-0.0480011  -0.04580301  0.03685523 64.83100954 -0.05288193]\n\n\nОкей, обычный градиентный спуск веса не обнуляет. Как быть?"
  },
  {
    "objectID": "xy_approximation.html",
    "href": "xy_approximation.html",
    "title": "Заметки",
    "section": "",
    "text": "Как аппроксимировать x*y c помощью нейросетки?\n\nimport numpy as np\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\n\n# Define the function\ndef f(x, y):\n    return x * y\n\n# Generate a grid of values\nx = np.linspace(-10, 10, 100)\ny = np.linspace(-10, 10, 100)\nx, y = np.meshgrid(x, y)\nz = f(x, y)\n\n# Create the plot\nfig = go.Figure(data=[go.Surface(z=z, x=x, y=y)])\n\n# Update layout for better visualization\nfig.update_layout(title='3D Plot of f(x, y) = x * y',\n                  scene=dict(xaxis_title='x',\n                             yaxis_title='y',\n                             zaxis_title='z = x * y'),\n                  width=800, height=800)\n\n# Show the plot\nfig.show()\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate the data\nx = np.linspace(-10, 10, 100)\ny = np.linspace(-10, 10, 100)\nx, y = np.meshgrid(x, y)\nz = x * y\n\n# Convert data to torch tensors\nx = torch.tensor(x.flatten(), dtype=torch.float32).view(-1, 1)\ny = torch.tensor(y.flatten(), dtype=torch.float32).view(-1, 1)\nz = torch.tensor(z.flatten(), dtype=torch.float32).view(-1, 1)\n\n# Concatenate x and y to create input features\ninputs = torch.cat((x, y), dim=1)\noutputs = z\n\n# Define the neural network\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(2, 64)\n        self.fc2 = nn.Linear(64, 64)\n        self.fc3 = nn.Linear(64, 1)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnet = Net()\n\n# Define loss function and optimizer\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(net.parameters(), lr=0.001)\n\n# Train the network\nepochs = 2\nfor epoch in range(epochs):\n    optimizer.zero_grad()\n    outputs_pred = net(inputs)\n    loss = criterion(outputs_pred, outputs)\n    loss.backward()\n    optimizer.step()\n\n    if (epoch + 1) % 500 == 0:\n        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n\n# Plot the results\nwith torch.no_grad():\n    z_pred = net(inputs).numpy().reshape(100, 100)\n\nplt.figure(figsize=(10, 8))\nplt.contourf(x.view(100, 100), y.view(100, 100), z_pred, levels=50, cmap='viridis')\nplt.colorbar()\nplt.title('Neural Network Approximation of f(x, y) = x * y')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n\n\nimport numpy as np\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\ndef f(x, y):\n    return x * y\n\n# Generate a grid of values for plotting\nx_plot = np.linspace(-1000, 1000, 1000)\ny_plot = np.linspace(-1000, 1000, 1000)\nx_plot, y_plot = np.meshgrid(x_plot, y_plot)\nz_plot = f(x_plot, y_plot)\n\n# Generate a grid of values for training\nx_train = np.linspace(-10, 10, 100)\ny_train = np.linspace(-10, 10, 100)\nx_train, y_train = np.meshgrid(x_train, y_train)\nz_train = f(x_train, y_train)\n\n# Prepare data for training\nX_train = np.stack((x_train.flatten(), y_train.flatten()), axis=-1)\ny_train_true = z_train.flatten()\n\n# Convert to PyTorch tensors\nX_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_tensor = torch.tensor(y_train_true, dtype=torch.float32).unsqueeze(1)\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(2, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 1)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nmodel = Net()\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 1000\nfor epoch in range(num_epochs):\n    optimizer.zero_grad()\n    outputs = model(X_tensor)\n    loss = criterion(outputs, y_tensor)\n    loss.backward()\n    optimizer.step()\n    if (epoch+1) % 200 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Generate predictions for plotting\nX_plot = np.stack((x_plot.flatten(), y_plot.flatten()), axis=-1)\nX_plot_tensor = torch.tensor(X_plot, dtype=torch.float32)\nz_pred = model(X_plot_tensor).detach().numpy().reshape(*z_plot.shape)\n\nfig = make_subplots(rows=1, cols=2, specs=[[{'type': 'surface'}, {'type': 'surface'}]])\n\nsurface1 = go.Surface(z=z_plot, x=x_plot, y=y_plot, showscale=False)\nfig.add_trace(surface1, row=1, col=1)\n\nsurface2 = go.Surface(z=z_pred, x=x_plot, y=y_plot, showscale=False)\nfig.add_trace(surface2, row=1, col=2)\n\nfig.update_layout(title='3D Plot of f(x, y) = x * y and Neural Network Approximation',\n                  scene=dict(xaxis_title='x',\n                             yaxis_title='y',\n                             zaxis_title='z = x * y'),\n                  scene2=dict(xaxis_title='x',\n                              yaxis_title='y',\n                              zaxis_title='z_pred'),\n                  width=1600, height=800)\n\nfig.show()\n\n\nlist(model.fc1.parameters())[0].shape\n\n\nX_tensor.shape\n\n\nlist(model.fc1.parameters())[0]\n\n\nimport numpy as np\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\ndef f(x, y):\n    return x * y\n\n# Generate a grid of values for plotting\nx_plot = np.linspace(-1000, 1000, 1000)\ny_plot = np.linspace(-1000, 1000, 1000)\nx_plot, y_plot = np.meshgrid(x_plot, y_plot)\nz_plot = f(x_plot, y_plot)\n\n# Generate a grid of values for training\nx_train = np.linspace(-10, 10, 100)\ny_train = np.linspace(-10, 10, 100)\nx_train, y_train = np.meshgrid(x_train, y_train)\nz_train = f(x_train, y_train)\n\n# Prepare data for training\nX_train = np.stack((x_train.flatten(), y_train.flatten()), axis=-1)\ny_train_true = z_train.flatten()\n\n# Convert to PyTorch tensors\nX_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_tensor = torch.tensor(y_train_true, dtype=torch.float32).unsqueeze(1)\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(2, 128)\n        self.fc2 = nn.Linear(128, 1)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\nmodel = Net()\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 1000\nfor epoch in range(num_epochs):\n    optimizer.zero_grad()\n    outputs = model(X_tensor)\n    loss = criterion(outputs, y_tensor)\n    loss.backward()\n    optimizer.step()\n    if (epoch+1) % 200 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Generate predictions for plotting\nX_plot = np.stack((x_plot.flatten(), y_plot.flatten()), axis=-1)\nX_plot_tensor = torch.tensor(X_plot, dtype=torch.float32)\nz_pred = model(X_plot_tensor).detach().numpy().reshape(*z_plot.shape)\n\nfig = make_subplots(rows=1, cols=2, specs=[[{'type': 'surface'}, {'type': 'surface'}]])\n\nsurface1 = go.Surface(z=z_plot, x=x_plot, y=y_plot, showscale=False)\nfig.add_trace(surface1, row=1, col=1)\n\nsurface2 = go.Surface(z=z_pred, x=x_plot, y=y_plot, showscale=False)\nfig.add_trace(surface2, row=1, col=2)\n\nfig.update_layout(title='3D Plot of f(x, y) = x * y and Neural Network Approximation',\n                  scene=dict(xaxis_title='x',\n                             yaxis_title='y',\n                             zaxis_title='z = x * y'),\n                  scene2=dict(xaxis_title='x',\n                              yaxis_title='y',\n                              zaxis_title='z_pred'),\n                  width=1600, height=800)\n\nfig.show()\n\n\nimport numpy as np\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\ndef f(x, y):\n    return x * y\n\n# Generate a grid of values for plotting\nx_plot = np.linspace(-1000, 1000, 1000)\ny_plot = np.linspace(-1000, 1000, 1000)\nx_plot, y_plot = np.meshgrid(x_plot, y_plot)\nz_plot = f(x_plot, y_plot)\n\n# Generate a grid of values for training\nx_train = np.linspace(-10, 10, 100)\ny_train = np.linspace(-10, 10, 100)\nx_train, y_train = np.meshgrid(x_train, y_train)\nz_train = f(x_train, y_train)\n\n# Prepare data for training\nX_train = np.stack((x_train.flatten(), y_train.flatten()), axis=-1)\ny_train_true = z_train.flatten()\n\n# Convert to PyTorch tensors\nX_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_tensor = torch.tensor(y_train_true, dtype=torch.float32).unsqueeze(1)\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(2, 128)\n        self.fc2 = nn.Linear(128, 1)\n\n    def forward(self, x):\n        x = torch.sigmoid(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\nmodel = Net()\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 10000\nfor epoch in range(num_epochs):\n    optimizer.zero_grad()\n    outputs = model(X_tensor)\n    loss = criterion(outputs, y_tensor)\n    loss.backward()\n    optimizer.step()\n    if (epoch+1) % 200 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Generate predictions for plotting\nX_plot = np.stack((x_plot.flatten(), y_plot.flatten()), axis=-1)\nX_plot_tensor = torch.tensor(X_plot, dtype=torch.float32)\nz_pred = model(X_plot_tensor).detach().numpy().reshape(*z_plot.shape)\n\nfig = make_subplots(rows=1, cols=2, specs=[[{'type': 'surface'}, {'type': 'surface'}]])\n\nsurface1 = go.Surface(z=z_plot, x=x_plot, y=y_plot, showscale=False)\nfig.add_trace(surface1, row=1, col=1)\n\nsurface2 = go.Surface(z=z_pred, x=x_plot, y=y_plot, showscale=False)\nfig.add_trace(surface2, row=1, col=2)\n\nfig.update_layout(title='3D Plot of f(x, y) = x * y and Neural Network Approximation',\n                  scene=dict(xaxis_title='x',\n                             yaxis_title='y',\n                             zaxis_title='z = x * y'),\n                  scene2=dict(xaxis_title='x',\n                              yaxis_title='y',\n                              zaxis_title='z_pred'),\n                  width=1600, height=800)\n\nfig.show()\n\n\nimport numpy as np\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\ndef f(x, y):\n    return x * y\n\n# Generate a grid of values for plotting\nx_plot = np.linspace(-1000, 1000, 1000)\ny_plot = np.linspace(-1000, 1000, 1000)\nx_plot, y_plot = np.meshgrid(x_plot, y_plot)\nz_plot = f(x_plot, y_plot)\n\n# Generate a grid of values for training\nx_train = np.linspace(-10, 10, 100)\ny_train = np.linspace(-10, 10, 100)\nx_train, y_train = np.meshgrid(x_train, y_train)\nz_train = f(x_train, y_train)\n\n# Prepare data for training\nX_train = np.stack((x_train.flatten(), y_train.flatten()), axis=-1)\ny_train_true = z_train.flatten()\n\n# Convert to PyTorch tensors\nX_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_tensor = torch.tensor(y_train_true, dtype=torch.float32).unsqueeze(1)\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(2, 1024)\n        self.fc2 = nn.Linear(1024, 1)\n\n    def forward(self, x):\n        x = torch.sigmoid(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\nmodel = Net()\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 10000\nfor epoch in range(num_epochs):\n    optimizer.zero_grad()\n    outputs = model(X_tensor)\n    loss = criterion(outputs, y_tensor)\n    loss.backward()\n    optimizer.step()\n    if (epoch+1) % 200 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Generate predictions for plotting\nX_plot = np.stack((x_plot.flatten(), y_plot.flatten()), axis=-1)\nX_plot_tensor = torch.tensor(X_plot, dtype=torch.float32)\nz_pred = model(X_plot_tensor).detach().numpy().reshape(*z_plot.shape)\n\nfig = make_subplots(rows=1, cols=2, specs=[[{'type': 'surface'}, {'type': 'surface'}]])\n\nsurface1 = go.Surface(z=z_plot, x=x_plot, y=y_plot, showscale=False)\nfig.add_trace(surface1, row=1, col=1)\n\nsurface2 = go.Surface(z=z_pred, x=x_plot, y=y_plot, showscale=False)\nfig.add_trace(surface2, row=1, col=2)\n\nfig.update_layout(title='3D Plot of f(x, y) = x * y and Neural Network Approximation',\n                  scene=dict(xaxis_title='x',\n                             yaxis_title='y',\n                             zaxis_title='z = x * y'),\n                  scene2=dict(xaxis_title='x',\n                              yaxis_title='y',\n                              zaxis_title='z_pred'),\n                  width=1600, height=800)\n\nfig.show()\n\n\nimport numpy as np\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\ndef f(x, y):\n    return x * y\n\n# Generate a grid of values for plotting\nx_plot = np.linspace(-1000, 1000, 1000)\ny_plot = np.linspace(-1000, 1000, 1000)\nx_plot, y_plot = np.meshgrid(x_plot, y_plot)\nz_plot = f(x_plot, y_plot)\n\n# Generate a grid of values for training\nx_train = np.linspace(-10, 10, 100)\ny_train = np.linspace(-10, 10, 100)\nx_train, y_train = np.meshgrid(x_train, y_train)\nz_train = f(x_train, y_train)\n\n# Prepare data for training\nX_train = np.stack((x_train.flatten(), y_train.flatten()), axis=-1)\ny_train_true = z_train.flatten()\n\n# Convert to PyTorch tensors\nX_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_tensor = torch.tensor(y_train_true, dtype=torch.float32).unsqueeze(1)\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(2, 1024)\n        self.fc2 = nn.Linear(1024, 256)\n        self.fc3 = nn.Linear(256, 1)\n\n    def forward(self, x):\n        x = torch.sigmoid(self.fc1(x))\n        x = torch.sigmoid(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nmodel = Net()\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 1000\nfor epoch in range(num_epochs):\n    optimizer.zero_grad()\n    outputs = model(X_tensor)\n    loss = criterion(outputs, y_tensor)\n    loss.backward()\n    optimizer.step()\n    if (epoch+1) % 200 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Generate predictions for plotting\nX_plot = np.stack((x_plot.flatten(), y_plot.flatten()), axis=-1)\nX_plot_tensor = torch.tensor(X_plot, dtype=torch.float32)\nz_pred = model(X_plot_tensor).detach().numpy().reshape(*z_plot.shape)\n\nfig = make_subplots(rows=1, cols=2, specs=[[{'type': 'surface'}, {'type': 'surface'}]])\n\nsurface1 = go.Surface(z=z_plot, x=x_plot, y=y_plot, showscale=False)\nfig.add_trace(surface1, row=1, col=1)\n\nsurface2 = go.Surface(z=z_pred, x=x_plot, y=y_plot, showscale=False)\nfig.add_trace(surface2, row=1, col=2)\n\nfig.update_layout(title='3D Plot of f(x, y) = x * y and Neural Network Approximation',\n                  scene=dict(xaxis_title='x',\n                             yaxis_title='y',\n                             zaxis_title='z = x * y'),\n                  scene2=dict(xaxis_title='x',\n                              yaxis_title='y',\n                              zaxis_title='z_pred'),\n                  width=1600, height=800)\n\nfig.show()\n\n\nimport numpy as np\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\ndef f(x, y):\n    return x * y\n\n# Generate a grid of values for plotting\nx_plot = np.linspace(-1000, 1000, 1000)\ny_plot = np.linspace(-1000, 1000, 1000)\nx_plot, y_plot = np.meshgrid(x_plot, y_plot)\nz_plot = f(x_plot, y_plot)\n\n# Generate a grid of values for training\nx_train = np.linspace(-10, 10, 100)\ny_train = np.linspace(-10, 10, 100)\nx_train, y_train = np.meshgrid(x_train, y_train)\nz_train = f(x_train, y_train)\n\n# Prepare data for training\nX_train = np.stack((x_train.flatten(), y_train.flatten()), axis=-1)\ny_train_true = z_train.flatten()\n\n# Convert to PyTorch tensors\nX_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_tensor = torch.tensor(y_train_true, dtype=torch.float32).unsqueeze(1)\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(2, 1024)\n        self.fc2 = nn.Linear(1024, 256)\n        self.fc3 = nn.Linear(256, 64)\n        self.fc4 = nn.Linear(64, 1)\n        \n\n    def forward(self, x):\n        x = torch.sigmoid(self.fc1(x))\n        x = torch.sigmoid(self.fc2(x))\n        x = torch.sigmoid(self.fc3(x))\n        x = self.fc4(x)\n        return x\n\nmodel = Net()\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 1000\nfor epoch in range(num_epochs):\n    optimizer.zero_grad()\n    outputs = model(X_tensor)\n    loss = criterion(outputs, y_tensor)\n    loss.backward()\n    optimizer.step()\n    if (epoch+1) % 200 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Generate predictions for plotting\nX_plot = np.stack((x_plot.flatten(), y_plot.flatten()), axis=-1)\nX_plot_tensor = torch.tensor(X_plot, dtype=torch.float32)\nz_pred = model(X_plot_tensor).detach().numpy().reshape(*z_plot.shape)\n\nfig = make_subplots(rows=1, cols=2, specs=[[{'type': 'surface'}, {'type': 'surface'}]])\n\nsurface1 = go.Surface(z=z_plot, x=x_plot, y=y_plot, showscale=False)\nfig.add_trace(surface1, row=1, col=1)\n\nsurface2 = go.Surface(z=z_pred, x=x_plot, y=y_plot, showscale=False)\nfig.add_trace(surface2, row=1, col=2)\n\nfig.update_layout(title='3D Plot of f(x, y) = x * y and Neural Network Approximation',\n                  scene=dict(xaxis_title='x',\n                             yaxis_title='y',\n                             zaxis_title='z = x * y'),\n                  scene2=dict(xaxis_title='x',\n                              yaxis_title='y',\n                              zaxis_title='z_pred'),\n                  width=1600, height=800)\n\nfig.show()\n\nEpoch [200/1000], Loss: 764.7434\nEpoch [400/1000], Loss: 576.9515\nEpoch [600/1000], Loss: 444.6906\nEpoch [800/1000], Loss: 347.4424\nEpoch [1000/1000], Loss: 274.2992\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  }
]